{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **House Price Prediction**"},{"metadata":{},"cell_type":"markdown","source":"In oder to build a model that will predict the house price in Ames, we followed a certain outline. We split the process into the following contents:\n1. Importing libraries and Loading data\n2. Exploratory data analysis\n3. different models experimented\n4. chosen model\n\nWe first needed to import most libraries needed and the data. Additional libraries will be imported as needed in the notebook. "},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np        # linear algebra\nimport pandas as pd       # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns     # data visualisation \n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import Lasso\n\n\nimport datetime     # datetime for calculating the age of things\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso\n\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data sets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Exploratory data analysis\n"},{"metadata":{},"cell_type":"markdown","source":"We first skeemed throught the variables and realised that we had two more columns than we thought. The Id column is not needed in our model therefore we can remove it. We will save the Id column first because we will need it for the submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We drop the ID columns in both datasets\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step was to look at the sale price, which is the varable we are investigating. Plotting a histogram showed that the data was negatively skewed distribution. Then it was normalised using log."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalising the data\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column \ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we to a look at the relationship between the different variables and sale price. We did this to help us identify and remove outliers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Outliers\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_train = train.sort_values(by = ['SalePrice'])\nq1,q3 = np.percentile(sorted_train['SalePrice'], [25,75])\niqr = q3-q1\nlower_bound = q1 - (1.5*iqr)\nupper_bound = q3 + (1.5*iqr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_bound","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_bound","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>lower_bound) & (train['SalePrice']<upper_bound)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data provided is already split into train and test. To clean the data all at once, we first combined the data, cleaned it and then split it again. This helped for both train and test data to be cleaned at once and in the same way."},{"metadata":{"trusted":true},"cell_type":"code","source":"#combining tests and train into one set to avoid code repetition\nntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data\nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visual representation of the misisng data\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n    \n#Special fills\n#zeros\nfor col in ('GarageYrBlt','GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0) #we don't fill GarageYrBlt we create GarageAge for it.\n            \nall_data['GarageAge'] = datetime.datetime.now().year - all_data['GarageYrBlt'] \nall_data.drop(columns=['GarageYrBlt'], inplace = True)\n            \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\n#median fill\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n#mode fill\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\n\n#explained by data_description.txt\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n\n#drops\nall_data = all_data.drop(['Utilities'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check remaining missing values if any \nall_data[col] = all_data[col].fillna(0)\n\nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step was to find correlation between variables and then with sale price."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we add first, second and basement surface area, we get total surface area of the house. As a result of this, we can drop the column second floor."},{"metadata":{"trusted":true},"cell_type":"code","source":"#New column TotalSF to keep some of the influence the soon to dropped column '2ndFlrSF'\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the ordinal and cartegorical values, we replaved them with numerical values to make them a numerical. This was done to each specific variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_map_landslope = {'Sev': 3, 'Mod': 2, 'Gtl': 1  }\nreplace_map_qual =  {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2,'Po': 1 }\nreplace_map_qual_na = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2,'Po': 1, 'None': 0  }\nreplace_map_finish = {'GLQ': 6,'ALQ': 5,'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None':0 }\nreplace_map_exposure = {'Gd': 4,'Av': 3,'Mn': 2, 'No': 1, 'None': 0 }\nreplace_map_yes_no = {'Y': 1, 'N': 0 }\nreplace_map_functional = { 'Typ': 8, 'Min1': 7, 'Min2':6, 'Mod':5, 'Maj1':4, 'Maj2': 3, 'Sev': 2, 'Sal':1 }\nreplace_map_garage_fin = { 'Fin': 3, 'RFn': 2 , 'Unf': 1, 'None': 0 }\nreplace_map_drive = { 'Y': 2 , 'P': 1 ,'N': 0 }\nreplace_map_pool = { 'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'None': 0 }\nreplace_map_fence = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'None': 0 }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dict_value(key, replace_dict):\n    '''Check if key is in dictionary\n       if not the function must do nothing\n       if the key is in the dictionary, the function must return the corresponding value'''\n    if key in replace_dict:\n        return replace_dict[key]\n    else:\n        return key","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ExterQual\nall_data['ExterQual'] = all_data['ExterQual'].apply(get_dict_value, replace_dict = replace_map_qual ) \nall_data['ExterQual'] = all_data['ExterQual'].astype('int')\n#ExterCond\nall_data['ExterCond'] = all_data['ExterCond'].apply(get_dict_value, replace_dict = replace_map_qual ) \nall_data['ExterCond'] = all_data['ExterCond'].astype('int')\n#LandSlope\nall_data['LandSlope'] = all_data['LandSlope'].apply(get_dict_value, replace_dict = replace_map_landslope ) \nall_data['LandSlope'] = all_data['LandSlope'].astype('int')\n#BsmtQual\nall_data['BsmtQual'] = all_data['BsmtQual'].apply(get_dict_value, replace_dict = replace_map_qual_na ) \nall_data['BsmtQual'] = all_data['BsmtQual'].astype('int')\n#BsmtCond\nall_data['BsmtCond'] = all_data['BsmtCond'].apply(get_dict_value, replace_dict = replace_map_qual_na ) \nall_data['BsmtCond'] = all_data['BsmtCond'].astype('int')\n#BsmtFinType1\nall_data['BsmtFinType1'] = all_data['BsmtFinType1'].apply(get_dict_value, replace_dict = replace_map_finish ) \nall_data['BsmtFinType1'] = all_data['BsmtFinType1'].astype('int')\n#BsmtFinType2\nall_data['BsmtFinType2'] = all_data['BsmtFinType2'].apply(get_dict_value, replace_dict = replace_map_finish ) \nall_data['BsmtFinType2'] = all_data['BsmtFinType2'].astype('int')\n#HeatingQC\nall_data['HeatingQC'] = all_data['HeatingQC'].apply(get_dict_value, replace_dict = replace_map_qual ) \nall_data['HeatingQC'] = all_data['HeatingQC'].astype('int')\n#CentralAir\nall_data['CentralAir'] = all_data['CentralAir'].apply(get_dict_value, replace_dict = replace_map_yes_no ) \nall_data['CentralAir'] = all_data['CentralAir'].astype('int')\n#KitchenQual\nall_data['KitchenQual'] = all_data['KitchenQual'].apply(get_dict_value, replace_dict = replace_map_qual_na ) \nall_data['KitchenQual'] = all_data['KitchenQual'].astype('int')\n#Functional\nall_data['Functional'] = all_data['Functional'].apply(get_dict_value, replace_dict = replace_map_functional ) \nall_data['Functional'] = all_data['Functional'].astype('int')\n#FireplaceQu\nall_data['FireplaceQu'] = all_data['FireplaceQu'].apply(get_dict_value, replace_dict = replace_map_qual_na ) \nall_data['FireplaceQu'] = all_data['FireplaceQu'].astype('int')\n#GarageFinish\nall_data['GarageFinish'] = all_data['GarageFinish'].apply(get_dict_value, replace_dict = replace_map_garage_fin) \nall_data['GarageFinish'] = all_data['GarageFinish'].astype('int')\n#GarageQual\nall_data['GarageQual'] = all_data['GarageQual'].apply(get_dict_value, replace_dict = replace_map_qual_na) \nall_data['GarageQual'] = all_data['GarageQual'].astype('int')\n#GarageCond\nall_data['GarageCond'] = all_data['GarageCond'].apply(get_dict_value, replace_dict = replace_map_qual_na) \nall_data['GarageCond'] = all_data['GarageCond'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we check our data again for skewness. This is because if the dataset is skewed, then the ML model would not a good job of predicting."},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])  #In case Box cox doesn't work\n#x = exp(log(lambda * transform + 1) / lambda)                    #To transform the values back","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some nominal values in our data that we need to get numerical values for them. We use .get_dummies to convert categorical variable into dummy/indicator variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now ready to build our model, we have cleaned our data enough. To start we split the data back to train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just incase we check for missing data one more time before we start with our model. And for some reason, Garage age has missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#missing data\ntest_na = (test.isnull().sum() / len(test)) * 100\ntest_na = test_na.drop(test_na[test_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame( {'Missing Ratio' :test_na} )\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['GarageAge'] = test['GarageAge'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc=RobustScaler()\nx=sc.fit_transform(train)\ntest=sc.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Lasso(alpha =0.001, random_state=1)\nmodel.fit(x,y_train)\npred=model.predict(test)\npreds=np.exp(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output=pd.DataFrame({'Id':test_ID.values, 'SalePrice':preds})\noutput.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}